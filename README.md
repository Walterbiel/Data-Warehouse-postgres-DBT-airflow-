
# Live Data Warehouse (PostgreSQL, dbt & Airflow)

Em parceria com a **3D Universe Creators** ‚Äî sorteio de logos em impress√£o 3D de Power BI e Python

---

## Vis√£o Geral

Este projeto demonstra a constru√ß√£o de um **Data Warehouse** completo utilizando **PostgreSQL** como banco relacional, **dbt** para transforma√ß√£o de dados e **Apache Airflow** para orquestra√ß√£o de pipelines. O objetivo √© atender √†s necessidades de integra√ß√£o, tratamento, an√°lise e visualiza√ß√£o de dados para √°reas de vendas, estoque e produ√ß√£o.

---

## Requisitos de Neg√≥cio

1. **Coleta e Integra√ß√£o de Dados**  
   - Integra√ß√£o autom√°tica com sistemas ERP para capturar tabelas de vendas, estoque e ordens de produ√ß√£o, com atualiza√ß√£o di√°ria ou em tempo real.  
   - Tratamento e padroniza√ß√£o (nomes de produtos, datas, filiais, centros de custo).  
   - Armazenamento em estrutura anal√≠tica (Data Warehouse ou Lakehouse).

2. **An√°lises Essenciais**  
   - Resumo di√°rio de vendas (quantidade, faturamento, ticket m√©dio por loja, canal e produto).  
   - Ruptura e giro de estoque (estoque zerado, tempo m√©dio de reposi√ß√£o, giro por categoria).  
   - Status da produ√ß√£o (ordens em aberto, em atraso, produ√ß√£o por linha/f√°brica).  
   - Comparativo Real √ó Meta (metas comerciais por equipe, linha de produto e per√≠odo).  
   - Alertas operacionais para desvios em vendas, estoque ou produ√ß√£o.

3. **Visualiza√ß√£o e A√ß√µes**  
   - Dashboard unificado com pain√©is interativos para vendas, estoque e produ√ß√£o, acess√≠vel em desktop e mobile.  
   - Filtros din√¢micos por per√≠odo, filial, produto, categoria e canal.  
   - Alertas automatizados (e-mail/WhatsApp) para eventos cr√≠ticos.  
   - Exporta√ß√£o de relat√≥rios (Excel/PDF) para reuni√µes estrat√©gicas.

---

## Como Come√ßar

### 1. Prepara√ß√£o do Projeto

```bash
# Clone o projeto e entre no diret√≥rio
git clone https://github.com/seu-usuario/seu-projeto.git
cd seu-projeto

# Crie um ambiente virtual
python -m venv .venv
source .venv/bin/activate

# Instale as depend√™ncias
pip install -r requirements.txt
```

### 2. Gerar uma chave para criptografia (opcional para dbt)

```bash
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

### 3. Crie as pastas necess√°rias

```bash
mkdir -p dags logs plugins dbt
```

---

## Orquestra√ß√£o com Apache Airflow (sem Docker)

### 1. Defina as vari√°veis de ambiente

```bash
export AIRFLOW_HOME=$(pwd)/airflow
AIRFLOW_VERSION=2.8.1
PYTHON_VERSION=$(python --version | cut -d " " -f2 | cut -d "." -f1,2)
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
```

### 2. Instale o Airflow

```bash
pip install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"
```

### 3. Inicialize o Airflow e crie o usu√°rio

```bash
airflow db init

airflow users create \
  --username admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@example.com \
  --password admin
```

### 4. Inicie o Airflow

Em dois terminais diferentes:

**Terminal 1 ‚Äì Scheduler**
```bash
source .venv/bin/activate
export AIRFLOW_HOME=$(pwd)/airflow
airflow scheduler
```

**Terminal 2 ‚Äì Webserver**
```bash
source .venv/bin/activate
export AIRFLOW_HOME=$(pwd)/airflow
airflow webserver --port 8080
```

Acesse no navegador: [http://localhost:8080](http://localhost:8080)

---

## Execu√ß√£o Local de Gera√ß√£o de Dados (FastAPI)

### 1. Execute o gerador de dados com Uvicorn

```bash
uvicorn gerador_de_dados.api_vendas_batch:app --reload --port 8002
```

---

## Execu√ß√£o com Docker (opcional)

### 1. Login no GitHub Container Registry (GHCR)

```bash
echo <SEU_GITHUB_TOKEN> | docker login ghcr.io -u <SEU_USUARIO_GITHUB> --password-stdin
```

> Gera√ß√£o do token: GitHub > Settings > Developer Settings > Personal Access Tokens  
> Permiss√µes necess√°rias: ‚úÖ `read:packages`

### 2. Subir os containers (Airflow + dbt)

```bash
docker compose --env-file .env up
```

---

## Observa√ß√µes

- O projeto est√° pronto para rodar **Airflow localmente ou via Docker**.
- Recomendado: PostgreSQL como banco de metadados do Airflow em produ√ß√£o.
- O SQLite √© usado aqui apenas para fins educacionais.

---

# üìÅ Estrutura das Tabelas na Camada Bronze

As seguintes tabelas est√£o dispon√≠veis no banco PostgreSQL, no schema `bronze`:

- `vendas`: informa√ß√µes de vendas realizadas.
- `devolucoes`: registros de devolu√ß√µes de vendas.
- `produtos`: cat√°logo de produtos com categoria e impostos.
- `lojas`: dados cadastrais das lojas.
- `vendedores`: cadastro de vendedores e datas importantes.

---

## üîß Configura√ß√£o do dbt (Data Build Tool)

Este projeto utiliza o **dbt** para organizar e transformar os dados da camada bronze at√© a gold. Abaixo est√£o os passos completos para configura√ß√£o e execu√ß√£o:

### 1. Inicializa√ß√£o do Projeto

```bash
dbt init vendas_dw
```

Siga os prompts e selecione o adaptador `Postgres`.

---

### 2. Estrutura Esperada do Projeto

```text
vendas_dw/
‚îú‚îÄ‚îÄ dbt_project.yml
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vendas.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ devolucoes.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ produtos.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lojas.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vendedores.sql
‚îÇ   ‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fct_vendas.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fct_devolucoes.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dim_lojas.sql
‚îÇ   ‚îú‚îÄ‚îÄ gold/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indicadores_vendas.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ produtos_mais_devolvidos.sql
‚îÇ   ‚îî‚îÄ‚îÄ _sources.yml
```

---

### 3. Configura√ß√£o do Profile

Crie ou edite o arquivo `~/.dbt/profiles.yml`:

```yaml
vendas_dw:
  target: dev
  outputs:
    dev:
      type: postgres
      host: localhost
      user: seu_usuario
      password: sua_senha
      port: 5432
      dbname: seu_banco
      schema: bronze
      threads: 4
```

---

### 4. Registro de Tabelas de Origem

`models/_sources.yml`:

```yaml
version: 2

sources:
  - name: bronze
    database: seu_banco
    schema: bronze
    tables:
      - name: vendas
      - name: devolucoes
      - name: produtos
      - name: lojas
      - name: vendedores
```

---

### 5. Exemplo de Modelo Bronze

`models/bronze/vendas.sql`:

```sql
SELECT * FROM {{ source('bronze', 'vendas') }}
```

---

### 6. Exemplo de Modelo Silver

`models/silver/fct_vendas.sql`:

```sql
WITH vendas_clean AS (
    SELECT
        id_venda,
        id_produto,
        preco,
        quantidade,
        data_venda::date AS data_venda,
        id_cliente,
        id_loja,
        id_vendedor,
        meio_pagamento,
        parcelamento
    FROM {{ ref('vendas') }}
)
SELECT * FROM vendas_clean
```

---

### 7. Exemplo de Modelo Gold

`models/gold/indicadores_vendas.sql`:

```sql
SELECT
    data_venda,
    id_loja,
    SUM(preco * quantidade) AS receita_total,
    COUNT(DISTINCT id_venda) AS qtd_vendas
FROM {{ ref('fct_vendas') }}
GROUP BY data_venda, id_loja
```

---

### 8. Executando o Projeto

Para compilar e rodar os modelos:

```bash
dbt run
```

Para validar a conex√£o e estrutura:

```bash
dbt debug
```

---

## üéØ Objetivos da Live

- Demonstrar como estruturar um DW do zero com dados transacionais.
- Aplicar boas pr√°ticas de modelagem dimensional.
- Apresentar o fluxo de camadas (bronze ‚Üí silver ‚Üí gold) com dbt.
- Mostrar indicadores de neg√≥cio em SQL a partir da camada gold.

---

## üß† Autor

**Walter Gonzaga**  
Data Architect | Engenheiro de Dados | Mentor  
[LinkedIn](https://www.linkedin.com/in/waltergonzaga)

---